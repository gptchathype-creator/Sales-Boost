# Варианты снижения задержки ответа в голосовом диалоге (~1 сек)

Сейчас ответ даёт наша LLM (OpenAI), и до первого ответа проходит ~5 сек. Ниже — только идеи и варианты, без реализации.

---

## 1. Оставить нашу LLM, но ускорить

- **Короткий промпт и история** — меньше токенов на ввод, быстрее ответ. Можно резать системный промпт и хранить в контексте только последние 2–4 реплики.
- **Меньше `max_tokens`** — например 120–150 вместо 220: модель быстрее «добивает» ответ.
- **Более быстрая модель** — если у провайдера есть «fast» вариант (например, другой размер или эндпоинт), переключиться на него только для голоса.
- **Стриминг ответа** — не ждать полный JSON, а стримить текст и как только накопилась фраза (предложение) — сразу отдавать в TTS. Первые слова могут пойти через ~1–2 сек, остальное — по мере генерации. Нужна доработка и бэкенда, и сценария (чанки уже поддерживаются по POST/WS).

---

## 2. Первая реплика без LLM (уже близко к «1 сек»)

- **Первое сообщение — шаблон**  
  Не вызывать LLM для самой первой фразы: подставлять шаблон вида «Здравствуйте! Я увидел объявление о [машина]. Он ещё доступен?» из `data/car.json`. Ответ по сути мгновенный, задержка только на сеть и TTS.
- Дальше с первой реплики пользователя уже вызывать LLM как сейчас.

---

## 3. «Подогрев» контекста пока пользователь говорит

- Идея: пока идёт ASR (пользователь говорит), в фоне уже отправить в LLM контекст (историю + «менеджер сейчас говорит») и получить один-два варианта ответа.
- Когда придёт финальный текст ASR, либо взять самый подходящий вариант, либо быстро его скорректировать одним коротким запросом.
- Минус: сложнее логика, возможны промахи по смыслу; выигрыш по времени не гарантирован при коротких репликах.

---

## 4. Использовать «нейрон» / AI со стороны Voximplant

- В Voximplant есть **NLU** (распознавание намерений), **AI** и интеграции. В документации и личном кабинете стоит посмотреть:
  - есть ли **готовый диалоговый AI / LLM** (свой или партнёрский), который вызывается из сценария без нашего бэкенда;
  - можно ли из сценария вызывать внешний API (наш сервер) только для «сложных» реплик, а простые (приветствие, «да/нет», цена) отдавать правилами или NLU.
- Если такой AI запускается рядом с медиа-серверами Voximplant, задержка до ответа может быть заметно меньше (сетевой круг до нашего Railway отпадёт для части реплик).
- Ограничение: наш виртуальный клиент (логика фаз, возражения, тон) зашит в нашем LLM; в Voximplant придётся либо воспроизвести эту логику правилами/NLU, либо оставить гибрид (простые ответы — Voximplant, сложные — наш API).

---

## 5. Гибрид: простые ответы — правила, сложные — наша LLM

- По намерению из ASR (или по ключевым словам) в сценарии выбирать:
  - **простое** (приветствие, «какая цена», «есть ли в наличии») → ответ из заготовленных фраз или простых правил в сценарии, без вызова нашего API → ответ почти мгновенный;
  - **сложное** (возражения, уточнения, длинный диалог) → POST на наш `/voice/dialog` и ответ от LLM.
- Так можно добиться «~1 сек» для большой доли реплик при сохранении качества на сложных.

---

## 6. Регион и сеть

- Размещать бэкенд (или прокси к OpenAI) **ближе к региону, где крутятся звонки Voximplant**, чтобы уменьшить RTT до OpenAI и обратно.
- Проверить, не добавляет ли лишней задержки прокси/балансировщик перед нашим приложением (особенно для POST `/voice/dialog`).

---

## Кратко

| Цель                         | Вариант                                              |
|-----------------------------|------------------------------------------------------|
| Быстрый первый ответ        | Шаблон первой фразы без LLM (п. 2)                  |
| Быстрый ответ в целом       | Стриминг LLM + TTS по чанкам (п. 1) или гибрид (п. 5) |
| Меньше зависимость от нас  | AI/NLU Voximplant для части реплик (п. 4, 5)         |
| Не трогать логику диалога   | Ускорить нашу LLM: короче промпт, меньше токенов (п. 1) |

«Передавать информацию пользователю заранее и почти не думать после его фразы» по смыслу ближе всего к **п. 2** (первая фраза — шаблон) и **п. 5** (простые ответы по правилам без вызова LLM).
